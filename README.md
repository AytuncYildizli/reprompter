# Reprompter

> Turn rough prompts into execution-grade prompts through an interactive interview.

![Version](https://img.shields.io/badge/version-5.1.0-0969da)
![License](https://img.shields.io/badge/license-MIT-2da44e)
![Claude Code](https://img.shields.io/badge/Claude%20Code-primary-111111)
![OpenClaw](https://img.shields.io/badge/OpenClaw-supported-7c3aed)
![LLM](https://img.shields.io/badge/Any%20Structured%20LLM-compatible-0ea5e9)

**Reprompter is not a static prompt template.** It interviews you, detects task complexity, picks execution mode (single-agent or team), and generates a prompt you can actually run.

Works with **Claude Code (primary)**, **OpenClaw**, or **any LLM that accepts structured prompts**.

---

## Before / After (the 15-second demo)

### Before (real rough input)

```text
uhh build a crypto dashboard, maybe coingecko data, add caching, test it too, don't break existing api
```

### After (generated by Reprompter)

```xml
<role>
Senior full-stack engineer for real-time dashboard systems and API integration
</role>

<context>
- Frontend + backend + tests required
- Data source: CoinGecko
- Existing API behavior must be preserved
- Execution mode: Team (Parallel) due to cross-layer scope
</context>

<motivation>
User-facing feature with reliability requirements. Prioritize correctness, resilience, and non-breaking integration.
</motivation>

<task>
Deliver a real-time crypto dashboard backed by a cache-aware CoinGecko proxy API,
with unit tests for both API and UI, without breaking existing API contracts.
</task>

<requirements>
- Build dashboard UI with loading/error/stale states
- Implement backend CoinGecko proxy with validation + cache TTL
- Preserve backward compatibility for existing API consumers
- Add deterministic unit tests for frontend and backend
</requirements>

<constraints>
- No direct client-side calls to CoinGecko
- No breaking changes to existing API response fields
- Mock external network boundaries in tests
</constraints>

<success_criteria>
- Dashboard updates on interval and handles failures gracefully
- Proxy endpoint returns normalized, validated data
- Existing API integration tests still pass
- New unit tests cover success, error, and stale-cache paths
</success_criteria>
```

### Quality jump (before vs after)

| Dimension | Before | After | Delta |
|---|---:|---:|---:|
| Clarity | 3/10 | 9/10 | +200% |
| Specificity | 2/10 | 9/10 | +350% |
| Structure | 1/10 | 10/10 | +900% |
| Constraints | 0/10 | 8/10 | +∞ |
| Verifiability | 1/10 | 9/10 | +800% |
| Decomposition | 2/10 | 9/10 | +350% |
| **Overall (weighted)** | **1.55/10** | **9.00/10** | **+481%** |

---

## How it works (interactive interview flow)

```text
Raw Prompt
   ↓
Quick Mode Gate
   ├─ Simple single-action task → generate immediately
   └─ Complex / multi-system task → AskUserQuestion interview
                                      ↓
                           Complexity + execution-mode detection
                                      ↓
                     Template selection + prompt generation
                                      ↓
                      Quality scoring (before vs after)
```

### 1) Quick Mode gate
If your prompt is truly simple (single action, single target), Reprompter skips interview latency.

### 2) AskUserQuestion interview (killer feature)
For anything non-trivial, Reprompter asks **structured, clickable questions**.

- Task type
- Execution mode (single / team / auto)
- Motivation (why this matters)
- Output format
- **Task-specific follow-ups** based on your actual wording

It doesn’t ask generic fluff. If you mention "tracking", it asks tracking questions. If you mention "signals", it asks signal delivery questions.

<details>
<summary><strong>Example interview payload (actual shape)</strong></summary>

```json
{
  "questions": [
    {
      "header": "Task Type",
      "question": "What type of task is this?",
      "options": [
        {"label": "Build Feature", "description": "Create new functionality"},
        {"label": "Fix Bug", "description": "Debug and resolve an issue"},
        {"label": "Refactor", "description": "Improve existing code structure"},
        {"label": "Multi-Agent/Swarm", "description": "Coordinate multiple agents"}
      ]
    },
    {
      "header": "Execution Mode",
      "question": "How should this be executed?",
      "options": [
        {"label": "Single Agent", "description": "One agent handles everything"},
        {"label": "Team (Parallel)", "description": "Split into specialized agents"},
        {"label": "Team (Sequential)", "description": "Pipeline handoffs"},
        {"label": "Let Reprompter decide", "description": "Auto-detect from complexity"}
      ]
    },
    {
      "header": "Motivation",
      "question": "Why does this matter?",
      "options": [
        {"label": "User-facing feature"},
        {"label": "Internal tooling"},
        {"label": "Bug fix / urgent"},
        {"label": "Exploration / research"},
        {"label": "Skip"}
      ]
    },
    {
      "header": "Data",
      "question": "What data source strategy should be used?",
      "options": [
        {"label": "Fresh/live"},
        {"label": "Cached"},
        {"label": "Fresh with cache fallback"},
        {"label": "Historical backfill"}
      ]
    }
  ]
}
```

</details>

### 3) Complexity detection + execution mode
Reprompter auto-detects when a task should be split across agents.

| Task signal | Suggested mode |
|---|---|
| Frontend + backend + tests | Team (Parallel) |
| Fetch → transform → deploy pipeline | Team (Sequential) |
| Single file/component change | Single Agent |
| Research + implementation | Team (Parallel) |

### 4) Generate + score
It picks a template, outputs a polished prompt (or team package), and shows a before/after score.

---

## Team Mode (full example)

This is where Reprompter stops being “prompt cleanup” and becomes orchestration.

### Input

```text
build a dashboard with real-time crypto prices, backend API from coingecko, and unit tests for both
```

### Interview result
- Task type: **Build Feature**
- Execution mode: **Let Reprompter decide**
- Motivation: **User-facing feature**
- Format: **XML**
- Data strategy: **Fresh with cache fallback**

### Auto-detection result
- Multiple systems detected (UI + API + test suite)
- Cross-layer dependencies detected
- ✅ **Execution mode chosen: Team (Parallel), 3 agents**

### Generated team brief

Reprompter writes a coordination brief to:

```text
/tmp/reprompter-brief-<timestamp>.md
```

<details>
<summary><strong>Team brief example</strong></summary>

```markdown
# Reprompter Team Brief

- Execution Mode: Team (Parallel)
- Overall Task: Real-time crypto dashboard with cache-aware backend and full unit coverage

## Agent Roles
1. Frontend Agent — dashboard UI, polling behavior, loading/error/stale states
2. Backend Agent — CoinGecko proxy API, schema validation, cache strategy
3. Tests Agent — deterministic unit tests for frontend + backend behavior

## Per-Agent Subtasks
### Frontend Agent
- Build dashboard view and update loop
- Consume internal API only
- Expose stable selectors for testability

### Backend Agent
- Implement /api/prices with normalization and validation
- Add short TTL cache and fallback behavior
- Preserve existing response compatibility

### Tests Agent
- Add unit tests for API success/error/stale-cache paths
- Add unit tests for UI loading/error/render states
- Mock all network boundaries

## Coordination Rules
- Backend publishes API contract first
- Frontend consumes contract without shape drift
- Tests use shared DTO definitions from backend contract
- Integration checkpoint before final merge

## Success Criteria
- Dashboard renders correctly under success and failure conditions
- API remains backward-compatible for existing consumers
- New tests pass and cover critical behavior paths
```

</details>

### Generated per-agent sub-prompts

Each agent gets a separate prompt with scoped responsibilities and handoff rules.

<details>
<summary><strong>Frontend Agent sub-prompt</strong></summary>

```xml
<role>
Senior frontend engineer specialized in real-time React dashboards
</role>

<context>
- Team mode: parallel with backend and test agents
- Backend will provide /api/prices contract
- Existing API consumers must not be broken
</context>

<task>
Implement dashboard UI for real-time crypto prices with loading, error, and stale states.
</task>

<requirements>
- Poll internal API on a defined interval
- Render deterministic states for loading/error/stale data
- Keep component boundaries test-friendly
</requirements>

<constraints>
- Do not call CoinGecko directly from client
- Do not modify existing API response fields
- Coordinate DTO usage with backend agent
</constraints>

<success_criteria>
- Dashboard updates correctly from internal API
- Error and stale states are clearly surfaced
- Test selectors/hooks are available for tests agent
</success_criteria>
```

</details>

<details>
<summary><strong>Backend Agent sub-prompt</strong></summary>

```xml
<role>
Senior backend engineer focused on API integration and resilient caching
</role>

<context>
- Team mode: parallel with frontend and test agents
- Data source: CoinGecko
- Existing API consumers rely on current response contract
</context>

<task>
Build a cache-aware /api/prices endpoint that proxies CoinGecko and returns normalized responses.
</task>

<requirements>
- Validate upstream payload shape before normalization
- Apply short TTL cache with fallback behavior
- Preserve backward-compatible response contract
</requirements>

<constraints>
- No client exposure of upstream internals
- No breaking response schema changes
- Define stable DTOs for frontend/tests consumption
</constraints>

<success_criteria>
- Endpoint handles success/failure/rate-limit paths gracefully
- Response schema remains compatible with existing consumers
- Contract documentation enables frontend + tests integration
</success_criteria>
```

</details>

<details>
<summary><strong>Tests Agent sub-prompt</strong></summary>

```xml
<role>
Senior test engineer specialized in deterministic unit and integration boundary tests
</role>

<context>
- Team mode: parallel with frontend and backend agents
- Backend publishes DTO contract
- Frontend exposes stable selectors/state hooks
</context>

<task>
Create robust unit tests for backend API behavior and frontend rendering states.
</task>

<requirements>
- Cover API success, failure, validation, and stale-cache branches
- Cover UI loading, success, error, and stale states
- Keep tests deterministic and fast
</requirements>

<constraints>
- Mock all external network boundaries
- No flaky timer-dependent assertions
- Align with backend DTO contract and frontend selectors
</constraints>

<success_criteria>
- Tests fail on contract drift and state regressions
- Critical behavior paths are covered for both layers
- Test suite is reproducible across environments
</success_criteria>
```

</details>

---

## Installation

### Claude Code (primary)

```bash
# from your project root
mkdir -p skills/reprompter
cp -R /path/to/reprompter/* skills/reprompter/
```

Claude Code discovers it from `skills/reprompter/SKILL.md`.

### OpenClaw

```bash
cp -R /path/to/reprompter /path/to/your-openclaw-workspace/skills/reprompter
```

### Any structured-prompt LLM

Use `SKILL.md` as your behavior spec and keep the XML/Markdown templates from `resources/templates/`.

---

## Quality Dimensions

Reprompter scores every transformation on six weighted dimensions.

| Dimension | Weight | What it checks |
|---|---:|---|
| Clarity | 20% | Is the task unambiguous? |
| Specificity | 20% | Are requirements concrete and scoped? |
| Structure | 15% | Is prompt structure complete and logical? |
| Constraints | 15% | Are boundaries explicit? |
| Verifiability | 15% | Can output be validated objectively? |
| Decomposition | 15% | Is work split cleanly (steps or agents)? |

**Overall score** = weighted average of the six dimensions.

---

## Templates

| Template | Use case |
|---|---|
| `feature-template` | New functionality |
| `bugfix-template` | Debug + fix |
| `refactor-template` | Structural cleanup |
| `testing-template` | Unit/integration test tasks |
| `api-template` | Endpoint/API work |
| `ui-component-template` | UI component implementation |
| `security-template` | Security hardening/audit tasks |
| `documentation-template` | Technical docs |
| `research-template` | Analysis / option exploration |
| `swarm-template` | Multi-agent coordination |
| `team-brief-template` | Team orchestration brief (generated artifact) |

> Templates live in `resources/templates/` (team brief is generated in runtime for team mode).

---

## v5.1 Modern Features

### 1) Think tool-aware prompting
Supports Claude 4.x workflows where heavy reasoning can use the dedicated think tool instead of over-prescriptive `<thinking>` blocks.

### 2) Context engineering awareness
Prompts are generated to complement runtime context (existing instructions, tools, memory), not blindly duplicate it.

### 3) Extended-thinking guidance
For models with deeper reasoning enabled, Reprompter favors outcome clarity over rigid step scripting.

### 4) Response prefilling support (API workflows)
Can suggest response prefills like `{` for JSON-first outputs in API usage.

### 5) Uncertainty handling section
Adds explicit permission for the model to ask for clarification instead of fabricating assumptions.

### 6) Motivation capture
Interview includes a “why this matters” step, then maps it into `<motivation>` so priority context survives execution.

---

## Credits

Built as a Claude Code skill, designed to run cleanly in Claude Code, OpenClaw, and structured-prompt LLM workflows.

## License

MIT — see [LICENSE](./LICENSE).
